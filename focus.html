<html>


<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8">
    <link rel="stylesheet" type="text/css" href="style.css">
    <title>Haiyan JIANG (she/her) - Research Associate @ NYUAD</title>
</head>


<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<h1 style="padding-left: 0.5em">Haiyan JIANG (she/her) - Postdoc Research Associate @ NYUAD</h1><hr>
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="focus.html">Research Focus</a></div>
<!--     <div class="menu-item"><a href="group.html">Research Group</a></div> -->
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="publication.html">Publications</a></div>
    <div class="menu-item"><a href="service.html">Professional Services</a></div>
    <div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
    <div class="menu-item"><a href="join.html">Join Us</a></div>
</td>


	
<td id="layout-content">

    <h1 style="margin-top: 0em">Research Focus</h1>

	As machine learning systems are increasingly deployed in real-world applications, 
	concerns about their security, robustness, and privacy have become critical. 
	To mitigate potential risks associated with these systems, our research focuses on security and privacy in 
	<strong>Federated Learning</strong> and 
	<strong>Machine Unlearning</strong>.
	
	The former investigates vulnerabilities in decentralized learning systems such as federated learning, 
	including backdoor attacks, adversarial threats, and privacy risks, while developing resilient defense mechanisms against malicious attacks.
	The latter focuses on protecting data privacy by enabling models to selectively forget specific training data upon request, 
	addressing privacy concerns such as model inversion and data leakage to ensure compliance with privacy regulations.

    <h2 style="margin-top: 0em">Key Research Areas</h2>
	<p><li><strong>Federated Learning Security</strong></li><br>
		Investigating security threats in decentralized learning systems such as federated learning, 
		including backdoor attacks and privacy threats, while developing robust countermeasures to enhance model security.
	</p>

	<p><li><strong>Machine Unlearning & Data Privacy</strong></li><br>
		Designing algorithms that enable models to forget specific training data,
		preventing model inversion attacks and data reconstruction, and ensuring privacy-preserving AI. 
	</p>

	
	Our research aims to mitigate potential security and privacy risks in modern AI systems, 
	ensuring they remain resilient, privacy-preserving, and ethically responsible in real-world deployments.


    <div>
        <h2><hr><a name="testing"></a>Federated Learning Security</h2>

	    Federated learning (FL) enables multiple participants to collaboratively train models without sharing their local private data, 
	    but it also introduces unique security and privacy vulnerabilities, including privacy risks and attacks. Our work in this area includes:
	    
    	</br></br>
        <ul>
	<p><li><strong>Backdoor Attacks & Defenses in FL</strong></li><br>
		Investigating how adversaries can stealthily implant hidden backdoors in FL models, 
		and developing effective detection and mitigation defense techniques.
	</p>
	
        <p><li><strong>Model Inversion Attacks</strong></li><br>
		Investigating how adversaries can reconstruct sensitive training data from shared model updates (weights or gradients), 
		and developing effective defense techniques to prevent the attackers from inferring private attributes of individual users, 
		even with encrypted or aggregated updates. 
	</p>
		
        <p><li><strong>Data Leakage from Gradients</strong></li><br>
		Examining how unencrypted gradient updates can expose sensitive information about underlying training data, 
		and designing robust mitigation strategies to prevent gradient-based data reconstruction attacks, 
		even in the presence of differential privacy techniques.
	</p>
		
	<p><li><strong>Secure Aggregation & Privacy Protection</strong></li><br>
		Strengthening FL protocols to prevent model inversion attacks, data leakage, and malicious updates.	
	</p>
	
        </ul></br>
    </div>

    <div>
        <h2><hr><a name="tml"></a>Machine Unlearning & Data Privacy</h2>
	    
	    As AI models continuously learn from vast amounts of data, 
	    the ability to selectively forget specific data's influence on trained models upon request 
	    is essential for privacy compliance (e.g., GDPR's "right to be forgotten"). 
	    However, even when only the trained model is accessible such as in Machine Learning-as-a-Service (MLaaS) settings, 
	    model inversion attacks can exploit pre-trained models to reconstruct original ensitive training data, 
	    raising concerns about privacy leakage in applications like facial recognition, medical diagnostics, and intelligent virtual assistants.
	    
	    Our research focuses on the following key areas:
	    

	<h3>Machine Unlearning</h3>
	    <ul>
	        <p><li><strong>Efficient Machine Unlearning Algorithms</strong></li><br>
			Developing methods that enable models to selectively remove the influence of specific training data 
			without requiring complete retraining the model, 
			maintaining model performance while ensuring compliance with privacy regulations.
		</p>

		<p><li><strong>Unlearning Inversion Attacks</strong></li><br>
			Investigating how adversaries can leverage model inversion and gradient inversion techniques 
			to infer feature and label information of an unlearned sample by analyzing changes between the original and unlearned models.
		</p>

		 <p><li><strong>Defending Against Inversion Attacks</strong></li><br>
			Designing robust defenses to prevent attackers from reconstructing sensitive training data, 
			mitigating risks associated with model inversion and gradient inversion attacks 
			in both white-box and black-box access scenarios.
		 </p>
		    
	    </ul></br>

    </div>

</td>
</tr>
</table>
</body>
</html>
